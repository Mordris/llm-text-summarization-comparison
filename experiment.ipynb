{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr0uwLQlg1ZM"
   },
   "source": [
    "# Introduction to Large Language Models\n",
    "\n",
    "## Text Summarization Experiment\n",
    "\n",
    "This notebook documents an experiment performing text summarization using different Large Language Models (LLMs) and exploring various text generation (decoding) strategies. The goal is to compare the quality of summaries produced by these different approaches using standard evaluation metrics.\n",
    "\n",
    "The experiment will be conducted on the CNN/DailyMail dataset, a widely used benchmark for abstractive summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETMWTEGzf6JG",
    "outputId": "d85c0e97-1ed7-437d-fe0c-26d620949432"
   },
   "outputs": [],
   "source": [
    "# @title Install required libraries\n",
    "\n",
    "# Install necessary packages for datasets, transformers, and evaluation metrics\n",
    "!pip install -q transformers datasets rouge_score sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TK6hq4sqg5Zq"
   },
   "source": [
    "## Loading and Inspecting the Dataset\n",
    "\n",
    "We will use the CNN/DailyMail dataset from the Hugging Face `datasets` library. This dataset contains news articles and corresponding human-written summaries (highlights). For this demonstration, we will load the dataset and inspect a single example from the test split to understand the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429,
     "referenced_widgets": [
      "b3631809524b46898c65a0114791e8d7",
      "c79d14c50ed04e79a6f86889d9c856f0",
      "f0b28b81847f431d86f7e6a98fd55ffd",
      "33635e6d6b4340749125b4f0a1a00ed0",
      "3e9dd29eef6648b3803c26a86297f6a8",
      "53fea35501d844298af14fe25f3ba3e3",
      "b21f1a38356d4b1bacd6f57c1aeed952",
      "057b1c25a7dc453b86b2f71a3ba74103",
      "90331f5292904ee29b92304403b043e6",
      "0e6a253590e44230a05b02886c40a66b",
      "028c183b14f44ce19138f9b226b22d6c",
      "b32dc7f99d824d95b7e5fd280dd0246c",
      "ed725443fece4be5a23af7ed79e23315",
      "75298606b4a84526aa57dae869cef2a5",
      "dd3ee6bdadb443bf9ae07c2bdfc64e53",
      "7df32c6dba1b4acb81b96d2037b7c683",
      "7c52873c547347ff9b5d803121b13b9b",
      "281f0696ed6e4b14a711c06b57f4dcb6",
      "f4050803c12b488085bd5f3766de3918",
      "1aec46d3416c4c65981aac86c9a30e64",
      "d553adc7b521452b940c9d58d9a1f0e5",
      "7cdc227e1c294f3785f9aa448ae9eadb",
      "481fcc1ad53c422ebbd49fb00d02af0f",
      "f316676398534eed992017e282fba038",
      "95cbd28a068341eebd7b4c88cd00f9d6",
      "bf87304293f546aaab62ddbe10b66160",
      "f219c3fb3f174862b05cada183371a3a",
      "89a113600cd744c19f6c6b2ed3b94b87",
      "e89e5658eeba42e48503db14347efef6",
      "ad75e5c79ab945539b9a9c4d93afd242",
      "0db9208fbb30481aae5964c8dbe7b4ca",
      "4d2143309341427ab728343819209399",
      "b0920172a6be4aea8b587bb1d6313b02",
      "033ce154978f424794f721a6a1bec72b",
      "070ec7f6f7044110a204a12906237715",
      "00a747a941ae4fc1850193cfceaf9ffd",
      "3f82d48b29e34fb3a932f2212f656c76",
      "faf65680f1c44ab6a6993fe782632869",
      "d185655a4f444f7d91d96f4efb5908f5",
      "35466867b6e14f28bc1535dd217bf4a1",
      "916832d51d1f46278f91badce3f5fed7",
      "1df66349713f4b70b303bf34fe80a2e3",
      "4c0887bc2e01406dbe8aa853a5db6e05",
      "fcfe6f014a834dfab33bb9f52a4f8bfc",
      "1e28df9489f747b9a795dadf11250883",
      "1c45cae7e5bc47b089282ab1bc975cdf",
      "a8476c135d584cd69b3596ab06cc9263",
      "4e57eb52301f4f8d884d8d9785aa0d99",
      "9c685435c3344e318f125f88d32e77d8",
      "609fba01868249f1ad8a56b56f1b8b98",
      "e2fdde56197b42e2b1a8d87ceff90f9a",
      "43d4c75915f64446bedc8cd7b67ef3e5",
      "0b63c416c2d544068260d64845f9e041",
      "44e52b422c6141c69e90dfdf66b1caa2",
      "0bf979c62e834b7594ec578095d079c1",
      "5cc720f4a757481e89558cbd8f2649c6",
      "195df055ef554eee89e177336f0a4b16",
      "e13bc207a6f64a9d8a64c8bb5d7a6cf4",
      "d7f8109e45154572a254c991aa23dbee",
      "d2807fd5e1a341188a579ce7b65fca52",
      "e404d5f639164e9884050c3ea9037cf1",
      "65309130fc504ab09aa9aafb2ea88922",
      "58154ed31fbd445a9e4b13fc4b22568d",
      "46a96f7dc01848dab914d9eb2a972685",
      "f42675d2702642b09ccc082cf4a7b777",
      "994e767d8f3944e89c32c3fb55d76eb8",
      "5de9a41491bb404598fce89a79092064",
      "8ec84f1f29444552a021cf9a2334456d",
      "59d62beb0417400894abd93280d6803a",
      "086f399a749e4722a8f49d9c85ec5f30",
      "7e7e2e71308a4cf3bca97a1a7e3637d1",
      "57dbbcce8e204e2682faff07dd53d351",
      "c78fe15f464e425cbe0bb4558e953403",
      "22bb430545ef416bb43bfe9190777ee9",
      "1b7fa1b2dcb64d418abf09382b7e40a2",
      "364fd3cddf624da891d3767355ef32f8",
      "05c1420071b046eeaf0a93d061118a17",
      "f9f15662c0384136aa7a69efa2ee1b2f",
      "393525b4254d4cc88f3c221066b955ac",
      "f5405b46ab824aa58e98e04d7ca5ad24",
      "a2d33164147e42419f36f3bd37e71bf6",
      "ad7fc77d34f84c5dbb13ead20eb14ca7",
      "e44488c9ab6d43f3923d3c0aa298eed8",
      "02d3eaa8d59f40dd806a0e96ea613ee0",
      "6c8442d97eb045e19fb5c52c1bc5bc91",
      "bb6f1b67b56941809a0634e50742b5b9",
      "12cb796757044cd38cadf56e1098f674",
      "be55a2d3924a449590b04d3722a22366",
      "6ba13b42badd4c75b6cd816623b43886",
      "143312ecafcb4b8e957760737033290c",
      "599c86f23bc7490ab94554910743925a",
      "7d53c3000423466da319324949e2f402",
      "e0e7b1a03aea496e933975e7e658fb3c",
      "85a22eaede4d49fc8cdbdc8f2a0459e4",
      "326628ce1f2a4711abeeda212426b052",
      "6acefc96dd5f47aeae53af1531989db6",
      "4fa5239bccdd448abcb31dd9eea7caa2",
      "dd404b00b4144ee98b59519dc118b2cf",
      "c8192fc18b734f5b8f2da6d017da674e"
     ]
    },
    "id": "q8-4c6HQg7BI",
    "outputId": "4c8a81e9-b582-4ae7-e273-ceab362f2354"
   },
   "outputs": [],
   "source": [
    "# @title Load dataset and show an example\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the specified version of the CNN/DailyMail dataset\n",
    "cnn_dm_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# Select the first example from the test set for demonstration\n",
    "example_article_data = cnn_dm_dataset[\"test\"][0]\n",
    "\n",
    "input_article = example_article_data[\"article\"]\n",
    "gold_summary = example_article_data[\"highlights\"] # Renamed from reference_summary\n",
    "\n",
    "# Display parts of the article and the gold summary\n",
    "print(\"--- Input Article (first 500 chars) ---\\n\", input_article[:500], \"...\\n\")\n",
    "print(\"--- Gold Summary ---\\n\", gold_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "remb7F1yg8lt"
   },
   "source": [
    "## Exploring Different Models and Decoding Strategies\n",
    "\n",
    "The assignment requires trying at least two different LLMs and at least three text generation (decoding) strategies.\n",
    "\n",
    "**Chosen Models:**\n",
    "1.  **BART:** A sequence-to-sequence model often used for abstractive summarization (`facebook/bart-large-cnn`).\n",
    "2.  **T5 (Text-to-Text Transfer Transformer):** Another powerful sequence-to-sequence model (`t5-small` for faster inference). T5 requires a specific input prefix like \"summarize:\".\n",
    "\n",
    "**Decoding Strategies:**\n",
    "These methods determine how the model generates tokens after training. We will explore:\n",
    "1.  **Greedy Search:** Selects the token with the highest probability at each step. Simple but can get stuck in local optima.\n",
    "2.  **Beam Search:** Explores multiple possible next tokens (`num_beams`) at each step, keeping track of the most promising sequences. Often produces better results than greedy search. We will use `num_beams=4`.\n",
    "3.  **Sampling:** Introduces randomness into the generation process. We'll use parameters like `do_sample=True`, `top_k` (considering only top K probable tokens), and `temperature` (controlling randomness). We will use `top_k=50` and `temperature=0.7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F49cZ3uag-Rk"
   },
   "outputs": [],
   "source": [
    "# @title Define the summarization function\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch # Added torch import\n",
    "\n",
    "def summarize_text_with_model(model, tokenizer, text_input, max_output_length=150, method=\"greedy\"):\n",
    "    \"\"\"\n",
    "    Generates a summary using a given model and tokenizer with different decoding methods.\n",
    "\n",
    "    Args:\n",
    "        model: The Hugging Face model for sequence-to-sequence generation.\n",
    "        tokenizer: The Hugging Face tokenizer corresponding to the model.\n",
    "        text_input (str): The input text to summarize.\n",
    "        max_output_length (int): The maximum length of the generated summary.\n",
    "        method (str): The decoding method to use ('greedy', 'beam', 'sampling').\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary.\n",
    "    \"\"\"\n",
    "    # Encode the input text\n",
    "    input_encoding = tokenizer.encode(\n",
    "        text_input,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024 # Using 1024 as a common max length for input context\n",
    "    )\n",
    "\n",
    "    # Move tensors to GPU if available (optional but good practice in Colab)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    input_encoding = input_encoding.to(device)\n",
    "\n",
    "    # Generate summary based on the specified method\n",
    "    if method == \"greedy\":\n",
    "        output_sequences = model.generate(\n",
    "            input_encoding,\n",
    "            max_length=max_output_length,\n",
    "            no_repeat_ngram_size=3, # Add a common generation parameter\n",
    "            early_stopping=True # Good practice for summarization\n",
    "        )\n",
    "    elif method == \"beam\":\n",
    "        output_sequences = model.generate(\n",
    "            input_encoding,\n",
    "            max_length=max_output_length,\n",
    "            num_beams=4, # Specified in text cell\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    elif method == \"sampling\":\n",
    "        output_sequences = model.generate(\n",
    "            input_encoding,\n",
    "            max_length=max_output_length,\n",
    "            do_sample=True,\n",
    "            top_k=50, # Specified in text cell\n",
    "            temperature=0.7, # Specified in text cell\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid decoding method: {method}. Choose from 'greedy', 'beam', 'sampling'.\")\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_summary = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrRmqbWkg_jp"
   },
   "source": [
    "## Generating Summaries with BART\n",
    "\n",
    "Now we will load the BART Large CNN model and generate summaries for our example article using the three decoding strategies defined earlier: Greedy Search, Beam Search, and Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244,
     "referenced_widgets": [
      "86193ea4d7cb4df6a98d0d98926daa5a",
      "dae14c1aa9a940108daaaeb8e510b97c",
      "7a10f7013cb1439e89c2e8469efa69fd",
      "33ed2f58530449cc890ec3b2851a32a6",
      "352df8a3f54b4fd385903e9a5c449cd2",
      "4d5758877f644f7e80cb68ed35f7ae67",
      "ee4028a7028446158a2fa61fbc808189",
      "23f9777141604614a9b62a14ba30f57e",
      "7585cd069b3d4ff29961384bad5bcce2",
      "eb553b17e1a94f459dbe5c6149fe1580",
      "b341790671164a7d9534a71b0c60b59c",
      "bcfeb7815ca147129053ec6773572b5f",
      "e0a2e0dff20a4c998b07dee13fa3f697",
      "6b536bfed22642b1b039a381afd22969",
      "c265a5a14f574c898b6cffd612664302",
      "4278db9db8e04f969002b59a075fbf0e",
      "ba34706049cd4e4798152b9632a1044e",
      "998302a61f0246548559b77d21e9d46a",
      "4aeea9e741494beb842b303172fcf239",
      "547436de45634b31a1ce456e8a2df986",
      "990deb3396064880b79f4bee4675b5f0",
      "0e92e2a053a44f6a87ac8a9260882135",
      "496c13b00bbb4193a659bb6945ad670e",
      "b8dd98df1bce4f0f896204f0bbf39ca7",
      "aeb8893364d046b6aea2db53a01f1368",
      "adfa47c3b5504df2b9b2ca22bdc21908",
      "dcc17d3c054f4739bb750c934360954d",
      "593484317ae44e06980b44f6262bcdd3",
      "5c0d3031b1bb42c081dbec0e18de5ec7",
      "5f85f203aa324f7a8c03dbf72bc4eabb",
      "5f13d804452b406c9a2aabc1510ae96e",
      "27dfb6a5e5584dc2a9253ca464a1f14a",
      "c55f81bb99b8490483bd433df01cf3f1",
      "dbd7df904e824f62b821424b0f2bae8a",
      "6bf7fc58e4814329b4239d2dc2c2d66f",
      "49d16ec352024f8596845a403fdf16e6",
      "57d005fe8d5f45ca867fafddea3d99c6",
      "e6f6d3d3f949414593a876a186669181",
      "41789a11c6244bd681cd97818fbb8d36",
      "3fae7793697644b29ef0ff5c74010038",
      "502590b8b6034f0b8e6baa8d389161f3",
      "4715ccb7046d4ad797ad4ebe78b0f3c8",
      "5ce71a0c3cc14c5c994b4d5fe52ecb4f",
      "e2c97b99914242c397ed4a235815fe1b",
      "55a6bfc1489d485c97987ab158eb45a7",
      "4e64fc1d6c964265a68437a1f5e348ff",
      "704ad4d071e44db4a05af90da1ee4cff",
      "839b03b84752421b9c2e8f683d7270f1",
      "ebfe12b1b7b34ec5bf281f490105eeee",
      "9a6cfa6353ec48bda5026a70adeda252",
      "f4f6cbc6304d457887b34d2dfced6898",
      "2065b3a9e9cf42528ec172310cfa6a4d",
      "2408d58d5d0840a9b14e082b7de61627",
      "096a69c1db804a418e4a93102df9dd92",
      "c9e80f0bd08a48e4851276eae42872d8",
      "3fe77a7b4d004d8a82ca83f85d55e4b1",
      "8021b0e469474ae5851f6c4dfd1823a1",
      "4db34f2e274149bfa4feddc3d7f2b1af",
      "fc4aeaf5ddbd4141922d7bdd1370b276",
      "5a416eefa2b841d6ac194b89c912b6e5",
      "bf181a222b154f85b831a676e9477053",
      "abf4f2434e8740f99362b39763ac9ca1",
      "06aae31e5d774169982a24f4281c68be",
      "8e275aa7ad8444c59efd45fb9b854fd7",
      "f9e27707eafd40ecbbdf947273c25f83",
      "e0e6ee6b830f4eca82fcb038282ea01c"
     ]
    },
    "id": "DqwNuAY4hA2f",
    "outputId": "56fdfa67-4e25-4283-c375-a837a6208b9b"
   },
   "outputs": [],
   "source": [
    "# @title Load BART model and generate summaries\n",
    "\n",
    "# Load BART tokenizer and model\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\") # Renamed tokenizer_bart\n",
    "bart_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\") # Renamed model_bart\n",
    "\n",
    "print(\"Generating summaries with BART...\")\n",
    "\n",
    "# Generate summaries using different methods\n",
    "bart_greedy_summary = summarize_text_with_model(bart_model, bart_tokenizer, input_article, method=\"greedy\") # Renamed bart_greedy\n",
    "bart_beam_summary = summarize_text_with_model(bart_model, bart_tokenizer, input_article, method=\"beam\")   # Renamed bart_beam\n",
    "bart_sample_summary = summarize_text_with_model(bart_model, bart_tokenizer, input_article, method=\"sampling\") # Renamed bart_sample\n",
    "\n",
    "print(\"BART summaries generated.\")\n",
    "\n",
    "# Optional: Display generated summaries\n",
    "# print(\"\\nBART (Greedy):\\n\", bart_greedy_summary)\n",
    "# print(\"\\nBART (Beam):\\n\", bart_beam_summary)\n",
    "# print(\"\\nBART (Sample):\\n\", bart_sample_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UeVcFw9hB3r"
   },
   "source": [
    "## Generating Summaries with T5\n",
    "\n",
    "Next, we load the T5 Small model. Remember that T5 uses a text-to-text format, so we need to prepend the input article with \"summarize: \". We will again generate summaries using Greedy Search, Beam Search, and Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "cfb69903e6e848cdbd7a7d55f806a81d",
      "68a08be58fb442dfbd89a34efd158b54",
      "43a97f6c3b7049b18ad2d726c133344e",
      "81298912fdda4813985ee398bb2f65d7",
      "deac57d5919c4f69b99174f0be206f16",
      "493c9b71c08b4dad87f020660b8740d7",
      "794d724ebd4741e8a7ab61e59ead0b67",
      "d14af75eb14a4835ae259f952d9b7a29",
      "7c4341b993ff4e7a9e4cdbd189a1e391",
      "88bcd810c2a241cd9bfbd218f157be65",
      "dd7ebcbe5d7c45ddb34c4fe9a3f93b31",
      "dce4088f9e274156adfecefe5601bca9",
      "e508354d68564ee9819d790a711e0f3f",
      "912eef793b084c83a2a9cad5d6470542",
      "b123d657dc5f4bfbb61b2031a20b8572",
      "82c54afce4394daba7ebf7f0b1c02078",
      "4011170ea7444806be3f0547afe45e21",
      "fe47cb43730c409b951756b5dc142490",
      "c7ea14dc465b4d83811a0029497595dc",
      "a9bfd8d347e2416b8ca10f047382d2e9",
      "387da1a86159498cb284c640aa96dea5",
      "06fc1ade0dd04e7ab5533cd5644d5c22",
      "b122ae0364b4458a951237cacda498fe",
      "d3bdb25a7db742dd9024b036fc70caf4",
      "b2f8fce36c1b4d5fb4d03d7ecf2fbea6",
      "4e3c4c9499864d6ca48628979316584d",
      "ffb77f2647c6494f8dc9f5df30d8b980",
      "d3cfdd26f453494abf96c5e28d1b7511",
      "259fdeba3b844abd9422e5833b8564e0",
      "51079abb95f04ae6ab6797877916e5ff",
      "02416a884b05470cb12d3207bc6a22fc",
      "3ba69a64e4004572b3057fc022b9c4e5",
      "96b7edbcc53346b5aabc76e41ee0d101",
      "5142afd52b424c73868fd32b9b38f4f0",
      "d8fc489dda434c11bd7eb498c7125076",
      "1153c9e58c424b5b8b6a057b7bba314c",
      "44bb579a6c1e46debf6b8d1ce6720e46",
      "a177ae071e9c4c49b6de750ecd4d7efe",
      "fcd98f72ada64305988acf122c387fe9",
      "c4751b7f1ec84cab9ccdafea721c6893",
      "53e93f5c99d84b12939de101b3ece75e",
      "0657d74b59a94239849d78d9b2baf351",
      "d5f5545ed87b459885f721d27bb9c3a4",
      "5b2055e9ca22454cba94e31ffe4b646e",
      "6f8b32eaa01343ca8a4be00a9bcfaf97",
      "62df603d8f1e46299b9c192147568cc7",
      "c8f96f8c5d33408da5fbb05286380d18",
      "f71f830e16514fb9960c71e494e9ae36",
      "5510bb0f8003454595c157d21892bc36",
      "b797e36d6d554f2f9e85c880706ac9e0",
      "9ad2b47de25047a7ba470d2d280f9018",
      "56e6e44bbc7841428f632c41ab77be9d",
      "2444d8658c1a41ee80addbd3e2178757",
      "db000f9170cb45f393f770fa09550909",
      "9a2e4e95f1044a8087e8b26f6b60f390",
      "57f4a0e8b7004703bfa514c917b198f3",
      "aac1483d80ce4c718f136fe785f277dd",
      "9aa0a046c56d4fe58846998d3d9dce31",
      "d965d043205f4ee781eb0b10c0587710",
      "ac207512681c4ea6ba0c63d58feeb7fe",
      "8486f1df639e462b86199a62a80f0eeb",
      "3e1a5f4340ee42b3b4b3758a7d3e94c2",
      "a3a4acab064d475881b8436817842ba8",
      "024ad7e6376042eba25104f237ef95ec",
      "c3a44e41ed86439990f6fb1fdf7a7f3b",
      "f53d3ca9b16541ce860151ec74241039"
     ]
    },
    "id": "3S06DRMqhC9j",
    "outputId": "7a32c740-df86-4a0e-8bd9-74ef48f8fa4c"
   },
   "outputs": [],
   "source": [
    "# @title Load T5 model and generate summaries\n",
    "\n",
    "# Load T5 tokenizer and model\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\") # Renamed tokenizer_t5\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\") # Renamed model_t5\n",
    "\n",
    "# Prepare input for T5 model (prepending the task)\n",
    "t5_input_text = \"summarize: \" + input_article # Renamed t5_input\n",
    "\n",
    "print(\"Generating summaries with T5...\")\n",
    "\n",
    "# Generate summaries using different methods\n",
    "t5_greedy_summary = summarize_text_with_model(t5_model, t5_tokenizer, t5_input_text, method=\"greedy\") # Renamed t5_greedy\n",
    "t5_beam_summary = summarize_text_with_model(t5_model, t5_tokenizer, t5_input_text, method=\"beam\")   # Renamed t5_beam\n",
    "t5_sample_summary = summarize_text_with_model(t5_model, t5_tokenizer, t5_input_text, method=\"sampling\") # Renamed t5_sample\n",
    "\n",
    "print(\"T5 summaries generated.\")\n",
    "\n",
    "# Optional: Display generated summaries\n",
    "# print(\"\\nT5 (Greedy):\\n\", t5_greedy_summary)\n",
    "# print(\"\\nT5 (Beam):\\n\", t5_beam_summary)\n",
    "# print(\"\\nT5 (Sample):\\n\", t5_sample_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5VkHez_hD-X"
   },
   "source": [
    "## Evaluating Summary Quality\n",
    "\n",
    "To quantitatively evaluate the quality of the generated summaries compared to the human-written gold summary, we will use standard text summarization metrics:\n",
    "\n",
    "1.  **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Compares an automatically produced summary to a set of reference summaries. It measures overlap of n-grams, word sequences, and word pairs.\n",
    "    *   ROUGE-1: Measures overlap of unigrams (single words).\n",
    "    *   ROUGE-2: Measures overlap of bigrams (two-word sequences).\n",
    "    *   ROUGE-L: Measures the longest common subsequence, capturing sentence-level structure similarity.\n",
    "    *   We will report the F1-score for ROUGE, which is the harmonic mean of precision and recall.\n",
    "2.  **BLEU (Bilingual Evaluation Understudy):** Originally developed for machine translation, it measures the precision of n-grams in the generated text compared to the reference text. While less ideal for abstractive summarization than ROUGE, it can still provide useful comparative insights.\n",
    "\n",
    "We will compute these metrics for each generated summary against the gold summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShuOR3KJhFGz"
   },
   "outputs": [],
   "source": [
    "# @title Define evaluation functions\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "def calculate_rouge_scores(predicted_summary, reference_summary): # Renamed compute_rouge\n",
    "    \"\"\"\n",
    "    Computes ROUGE-1, ROUGE-2, and ROUGE-L F1 scores.\n",
    "\n",
    "    Args:\n",
    "        predicted_summary (str): The generated summary.\n",
    "        reference_summary (str): The human-written gold summary.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with ROUGE scores (rouge1, rouge2, rougeL).\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_summary, predicted_summary) # Note: rouge_score library takes ref, pred order\n",
    "\n",
    "    # Extract F1 scores\n",
    "    return {key: scores[key].fmeasure for key in ['rouge1', 'rouge2', 'rougeL']}\n",
    "\n",
    "def calculate_bleu_score(predicted_summaries, reference_summaries): # Renamed compute_bleu, expecting lists\n",
    "    \"\"\"\n",
    "    Computes the corpus BLEU score.\n",
    "\n",
    "    Args:\n",
    "        predicted_summaries (list): A list of generated summaries.\n",
    "        reference_summaries (list): A list of reference summaries (each element can be a single string or a list of reference strings).\n",
    "\n",
    "    Returns:\n",
    "        float: The corpus BLEU score.\n",
    "    \"\"\"\n",
    "    # Ensure reference_summaries is a list of lists if needed (sacrebleu expects [[ref1], [ref2], ...])\n",
    "    # For a single reference per prediction, it should be [[ref1_pred1], [ref1_pred2], ...]\n",
    "    formatted_references = [[ref] for ref in reference_summaries]\n",
    "    return corpus_bleu(predicted_summaries, formatted_references).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_PcGKNphGN1"
   },
   "source": [
    "## Computing and Presenting Results\n",
    "\n",
    "Now we will compute the ROUGE and BLEU scores for each generated summary and organize the results into a table for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 882
    },
    "id": "cZ9DmkMDhHhQ",
    "outputId": "79998f0e-5333-4ca8-e569-ba7d4494fd13"
   },
   "outputs": [],
   "source": [
    "# @title Compute metrics and display results table\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary to store evaluation results\n",
    "evaluation_results_dict = { # Renamed results\n",
    "    \"Model\": [],\n",
    "    \"Decoding Strategy\": [], # Renamed Decoding\n",
    "    \"Generated Summary\": [], # Renamed Summary\n",
    "    \"ROUGE-1 (F1)\": [], # Added (F1)\n",
    "    \"ROUGE-2 (F1)\": [], # Added (F1)\n",
    "    \"ROUGE-L (F1)\": [], # Added (F1)\n",
    "    \"BLEU Score\": [], # Renamed BLEU\n",
    "}\n",
    "\n",
    "# Store the generated summaries with identifiable labels\n",
    "generated_summaries_map = { # Renamed summaries\n",
    "    \"BART (Greedy)\": bart_greedy_summary, # Using new variable names\n",
    "    \"BART (Beam)\": bart_beam_summary,\n",
    "    \"BART (Sampling)\": bart_sample_summary,\n",
    "    \"T5 (Greedy)\": t5_greedy_summary,\n",
    "    \"T5 (Beam)\": t5_beam_summary,\n",
    "    \"T5 (Sampling)\": t5_sample_summary,\n",
    "}\n",
    "\n",
    "# Iterate through the summaries, compute metrics, and fill the results dictionary\n",
    "for label, summary_text in generated_summaries_map.items():\n",
    "    model_name, decoding_method = label.split()\n",
    "    decoding_method = decoding_method[1:-1] # Remove parentheses\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = calculate_rouge_scores(summary_text, gold_summary) # Using new function and variable names\n",
    "\n",
    "    # Compute BLEU score (Note: BLEU is typically corpus-level, but we compute for single example here for comparison)\n",
    "    # We pass the summaries as lists as expected by the function\n",
    "    bleu_score = calculate_bleu_score([summary_text], [gold_summary]) # Using new function and variable names\n",
    "\n",
    "    # Append results to the dictionary\n",
    "    evaluation_results_dict[\"Model\"].append(model_name)\n",
    "    evaluation_results_dict[\"Decoding Strategy\"].append(decoding_method)\n",
    "    evaluation_results_dict[\"Generated Summary\"].append(summary_text)\n",
    "    evaluation_results_dict[\"ROUGE-1 (F1)\"].append(round(rouge_scores[\"rouge1\"], 3))\n",
    "    evaluation_results_dict[\"ROUGE-2 (F1)\"].append(round(rouge_scores[\"rouge2\"], 3))\n",
    "    evaluation_results_dict[\"ROUGE-L (F1)\"].append(round(rouge_scores[\"rougeL\"], 3))\n",
    "    evaluation_results_dict[\"BLEU Score\"].append(round(bleu_score, 2)) # BLEU scores are often reported with fewer decimal places\n",
    "\n",
    "# Create a pandas DataFrame from the results dictionary\n",
    "results_dataframe = pd.DataFrame(evaluation_results_dict) # Renamed df\n",
    "\n",
    "# Display the DataFrame with wide columns to see the summaries\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(\"--- Evaluation Results ---\")\n",
    "display(results_dataframe) # Use display() in Colab for better formatting\n",
    "\n",
    "# Optionally display the gold summary again for easy comparison\n",
    "print(\"\\n--- Gold Summary for Reference ---\")\n",
    "print(gold_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZR5GyslhJC3"
   },
   "source": [
    "## Discussion and Findings\n",
    "\n",
    "Based on the results table and the generated summaries, we can make the following observations for this specific example:\n",
    "\n",
    "*   **Model Comparison:** BART Large CNN generally appears to produce higher ROUGE scores than T5 Small on this example. This is expected, as BART Large CNN is a larger model specifically fine-tuned for summarization on the CNN/DailyMail dataset, while T5 Small is a much smaller, general-purpose text-to-text model.\n",
    "*   **Decoding Strategy Comparison (within BART):** For BART, Beam Search (num_beams=4) yielded slightly higher ROUGE scores compared to Greedy Search and Sampling. Greedy Search is often the fastest but can produce less coherent summaries. Sampling introduces variation but might not always find the highest scoring sequence according to these metrics.\n",
    "*   **Decoding Strategy Comparison (within T5):** Similar to BART, Beam Search seems to perform best for T5 in terms of ROUGE scores on this example. The performance difference between strategies might be less pronounced with a smaller model like T5-small.\n",
    "*   **BLEU vs. ROUGE:** The BLEU scores generally follow the same trend as ROUGE, showing better performance for BART compared to T5-small, and often slightly favoring Beam Search. It's important to remember BLEU's focus on n-gram precision, which can sometimes be a less reliable indicator of abstractive summary quality than ROUGE.\n",
    "*   **Qualitative Look:** While the metrics provide quantitative comparison, reading the generated summaries is crucial. BART's summaries are likely more fluent and coherent due to the model's architecture and training data. The T5-small summaries might be more concise but potentially less detailed or grammatically awkward. Beam Search and Sampling summaries from BART might show subtle differences in phrasing compared to Greedy Search.\n",
    "*   **Limitations:** This analysis is based on only *one* example from the dataset. A comprehensive evaluation would require testing on a larger sample of the test set and potentially analyzing summary length, factual accuracy, and other qualitative aspects. The choice of decoding parameters (beam width, temperature, top_k, max_length) also significantly impacts the results.\n",
    "\n",
    "In conclusion, for this summarization task on the CNN/DailyMail dataset, the larger, domain-specific BART model generally outperforms the smaller, general-purpose T5-small model. Among decoding strategies, Beam Search often provides a good balance between quality and computational cost compared to simple Greedy Search or potentially less controlled Sampling, based on standard metrics like ROUGE."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
